{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VceFwaQJMctO",
    "outputId": "9de001ce-a13a-4114-9c1f-8154d33981e4",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuan/miniconda3/envs/shap/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arr_0', 'arr_1']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.image as img\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import timm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "data = np.load(\"./Wafer_Map_Datasets.npz\")\n",
    "print(data.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XcJlh7isMctO",
    "outputId": "8b8be51e-048b-4f64-dea1-09d01321fa98",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練集筆數: 26610\n",
      "驗證集筆數: 5702\n",
      "測試集筆數: 5703\n",
      "(26610, 52, 52)\n"
     ]
    }
   ],
   "source": [
    "arrays = data['arr_0']\n",
    "label = data['arr_1']\n",
    "# print(arrays.shape)\n",
    "arrays = np.reshape(arrays,(38015,52*52))\n",
    "combine_array = np.concatenate([label,arrays],axis=1)\n",
    "# print(combine_array.shape)\n",
    "\n",
    "np.random.shuffle(combine_array)\n",
    "\n",
    "# 設定每個集合的大小\n",
    "total_rows = combine_array.shape[0]  # 總行數\n",
    "train_size = int(total_rows * 0.7)  # 訓練集大小 (70%)\n",
    "val_size = int(total_rows * 0.15)   # 驗證集大小 (15%)\n",
    "test_size = total_rows - train_size - val_size  # 測試集大小 (剩下的行數)\n",
    "\n",
    "# 分割數據集\n",
    "train_set = combine_array[:train_size]  # 前 70% 為訓練集\n",
    "val_set = combine_array[train_size:train_size + val_size]  # 接下來的 15% 為驗證集\n",
    "test_set = combine_array[train_size + val_size:]  # 剩下的 15% 為測試集\n",
    "\n",
    "print(\"訓練集筆數:\", train_set.shape[0])\n",
    "print(\"驗證集筆數:\", val_set.shape[0])\n",
    "print(\"測試集筆數:\", test_set.shape[0])\n",
    "\n",
    "train_X = train_set[:,8: ]\n",
    "train_Y = train_set[:, :8]\n",
    "val_X = val_set[:,8: ]\n",
    "val_Y = val_set[:, :8]\n",
    "test_X = test_set[:,8: ]\n",
    "test_Y = test_set[:, :8]\n",
    "\n",
    "train_X = np.reshape(train_X,(-1,52,52))\n",
    "val_X = np.reshape(val_X,(-1,52,52))\n",
    "test_X = np.reshape(test_X,(-1,52,52))\n",
    "\n",
    "print(train_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lb4DqOAJMctP",
    "outputId": "e6e6d954-95b0-418e-fffb-355e66853df2",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  1  2  3  4  5  6  7\n",
      "1  0  1  0  0  0  1  0    1371\n",
      "            1  0  0  0     726\n",
      "0  1  1  0  0  0  0  0     725\n",
      "   0  0  0  1  0  1  0     719\n",
      "      1  0  0  0  1  0     719\n",
      "1  0  0  1  1  0  0  0     717\n",
      "                  1  0     715\n",
      "         0  0  0  0  0     714\n",
      "0  1  1  0  1  0  0  0     714\n",
      "   0  0  0  1  0  0  0     712\n",
      "   1  0  0  0  0  0  0     711\n",
      "            1  0  0  0     710\n",
      "   0  0  1  0  0  0  0     710\n",
      "                  1  0     710\n",
      "            1  0  0  0     710\n",
      "   1  0  1  0  0  0  0     707\n",
      "                  1  0     707\n",
      "   0  0  0  0  0  1  0     707\n",
      "1  0  0  1  0  0  1  0     705\n",
      "0  1  0  0  1  0  1  0     701\n",
      "         1  1  0  1  0     699\n",
      "   0  1  0  1  0  1  0     698\n",
      "1  0  1  0  0  0  0  0     697\n",
      "      0  0  1  0  1  0     695\n",
      "                  0  0     694\n",
      "0  0  1  0  1  0  0  0     692\n",
      "            0  0  0  0     690\n",
      "      0  1  1  0  1  0     688\n",
      "1  0  0  0  0  0  1  0     687\n",
      "0  0  0  0  0  0  0  0     686\n",
      "1  0  1  0  1  0  1  0     681\n",
      "      0  1  0  0  0  0     679\n",
      "0  1  1  0  1  0  1  0     678\n",
      "      0  1  1  0  0  0     676\n",
      "      1  0  0  0  1  0     674\n",
      "      0  0  0  0  1  0     674\n",
      "   0  0  0  0  0  0  1     606\n",
      "               1  0  0     106\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 看各 label 個數\n",
    "print(pd.DataFrame(train_Y).value_counts())\n",
    "\n",
    "def decode_to_38bit(input_ndarray):\n",
    "    mapping = {\n",
    "        \"00000000\": 0,\n",
    "        \"10000000\": 1,\n",
    "        \"01000000\": 2,\n",
    "        \"00100000\": 3,\n",
    "        \"00010000\": 4,\n",
    "        \"00001000\": 5,\n",
    "        \"00000100\": 6,\n",
    "        \"00000010\": 7,\n",
    "        \"00000001\": 8,\n",
    "        \"10100000\": 9,\n",
    "        \"10010000\": 10,\n",
    "        \"10001000\": 11,\n",
    "        \"10000010\": 12,\n",
    "        \"01100000\": 13,\n",
    "        \"01010000\": 14,\n",
    "        \"01001000\": 15,\n",
    "        \"01000010\": 16,\n",
    "        \"00101000\": 17,\n",
    "        \"00100010\": 18,\n",
    "        \"00011000\": 19,\n",
    "        \"00010010\": 20,\n",
    "        \"00001010\": 21,\n",
    "        \"10101000\": 22,\n",
    "        \"10100010\": 23,\n",
    "        \"10011000\": 24,\n",
    "        \"10010010\": 25,\n",
    "        \"10001010\": 26,\n",
    "        \"01101000\": 27,\n",
    "        \"01100010\": 28,\n",
    "        \"01011000\": 29,\n",
    "        \"01010010\": 30,\n",
    "        \"01001010\": 31,\n",
    "        \"00101010\": 32,\n",
    "        \"00011010\": 33,\n",
    "        \"10101010\": 34,\n",
    "        \"10011010\": 35,\n",
    "        \"01101010\": 36,\n",
    "        \"01011010\": 37\n",
    "    }\n",
    "\n",
    "    n = input_ndarray.shape[0]  # 行數\n",
    "    decoded_ndarray = np.zeros(n, dtype=int)\n",
    "\n",
    "    # 逐行解碼\n",
    "    for i, row in enumerate(input_ndarray):\n",
    "        # 將每一行轉換為字典的鍵，並獲得對應的索引\n",
    "        input_8bit = ''.join(row.astype(str))  # 把整數轉為字串並拼接\n",
    "\n",
    "        number = mapping[input_8bit]  # 獲取對應的數字索引\n",
    "        decoded_ndarray[i] = number\n",
    "\n",
    "    return decoded_ndarray\n",
    "\n",
    "# label 轉成 one hot encoding\n",
    "train_Y = decode_to_38bit(train_Y)\n",
    "val_Y = decode_to_38bit(val_Y)\n",
    "test_Y = decode_to_38bit(test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 569
    },
    "id": "is46nE1mF1Wf",
    "outputId": "981add30-fdf6-471b-cb6f-9113bc0122a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52, 52)\n",
      "3\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 用來測試縮放\n",
    "test_image = np.reshape(test_X[7],(52,52))\n",
    "print(test_image.shape)\n",
    "print(test_Y[7])\n",
    "\n",
    "def resize_ndarray(input_array, target_height, target_width):\n",
    "    # 使用 OpenCV 的 resize 函數進行調整\n",
    "    resized_array = cv2.resize(input_array, (target_width, target_height), interpolation=cv2.INTER_NEAREST)\n",
    "    return resized_array\n",
    "\n",
    "resize_image = resize_ndarray(test_image, 57,60)\n",
    "\n",
    "# 創建畫布，並劃分為 1 行 2 列\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# 第一張圖片\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(test_image)\n",
    "plt.title(\"before\")\n",
    "\n",
    "# 第二張圖片\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(resize_image)\n",
    "plt.title(\"after\")\n",
    "\n",
    "# 顯示圖片\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Bu4z7D2RMctV",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 自定義一個簡單的 Dataset 類\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self,mydata,mylabel):\n",
    "        # 定義數據\n",
    "        self.data = mydata\n",
    "        self.labels = mylabel\n",
    "\n",
    "    def __len__(self):\n",
    "        # 返回數據集的大小\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 返回指定 index 的數據和標籤\n",
    "        return self.data[index], self.labels[index]\n",
    "\n",
    "# --- 使用本地 EfficientNet 模型的 CategoricalCNN ---\n",
    "from transformers import EfficientNetForImageClassification, EfficientNetConfig\n",
    "\n",
    "class CategoricalCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    專為二維類別資料（如晶圓圖）設計的 CNN。\n",
    "    使用本地的 EfficientNet 模型。\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, num_categories=4, embedding_dim=16, model_path='./efficientnet_b0'):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_categories = num_categories\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # 1. 嵌入層，用於學習晶圓圖中各個類別的向量表示\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=num_categories, \n",
    "            embedding_dim=embedding_dim\n",
    "        )\n",
    "\n",
    "        # 2. 載入本地 EfficientNet 模型\n",
    "        try:\n",
    "            # 載入配置\n",
    "            config = EfficientNetConfig.from_pretrained(model_path)\n",
    "            \n",
    "            # 修改配置以適應我們的需求\n",
    "            config.num_channels = embedding_dim  # 修改輸入通道數\n",
    "            config.num_labels = num_classes      # 修改輸出類別數\n",
    "            \n",
    "            # 載入模型\n",
    "            self.backbone = EfficientNetForImageClassification.from_pretrained(\n",
    "                model_path, \n",
    "                config=config,\n",
    "                ignore_mismatched_sizes=True  # 忽略尺寸不匹配的層\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"載入本地模型失敗: {e}\")\n",
    "            print(\"回退到 timm 模型...\")\n",
    "            # 回退方案：使用 timm\n",
    "            import timm\n",
    "            self.backbone = timm.create_model(\n",
    "                'efficientnet_b0',\n",
    "                pretrained=False,\n",
    "                features_only=True,\n",
    "                in_chans=embedding_dim\n",
    "            )\n",
    "            # 添加分類頭\n",
    "            num_features = self.backbone.feature_info.channels(-1)\n",
    "            self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "            self.classifier = nn.Linear(num_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x 的輸入形狀: (batch_size, height, width)，其值為整數類別\n",
    "        x = x.long()\n",
    "\n",
    "        # 通過嵌入層處理\n",
    "        # 輸出形狀: (batch_size, height, width, embedding_dim)\n",
    "        embedded_x = self.embedding(x)\n",
    "\n",
    "        # 調整維度以匹配 CNN 的輸入格式 (N, C, H, W)\n",
    "        # 輸出形狀: (batch_size, embedding_dim, height, width)\n",
    "        embedded_x = embedded_x.permute(0, 3, 1, 2)\n",
    "\n",
    "        # 通過 EfficientNet\n",
    "        if hasattr(self, 'classifier'):  # timm 回退方案\n",
    "            features = self.backbone(embedded_x)\n",
    "            last_feature_map = features[-1]\n",
    "            pooled_features = self.global_pool(last_feature_map).flatten(1)\n",
    "            output = self.classifier(pooled_features)\n",
    "        else:  # Hugging Face 模型\n",
    "            outputs = self.backbone(embedded_x)\n",
    "            output = outputs.logits\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lQKT-G7-bZN0",
    "outputId": "936a5fa9-2366-4fce-b384-60cb8b885cb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在將晶圓圖大小從 (52, 52) 調整為 (52, 52)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "調整訓練集大小: 100%|██████████| 26610/26610 [00:00<00:00, 474814.75it/s]\n",
      "調整驗證集大小: 100%|██████████| 5702/5702 [00:00<00:00, 579428.74it/s]\n",
      "調整測試集大小: 100%|██████████| 5703/5703 [00:00<00:00, 568524.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "大小調整完成。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用設備: mps\n",
      "模型將儲存至: output/Custom_EfficientNet_model.pth\n",
      "使用本地 EfficientNet 模型: ./efficientnet_b0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EfficientNetForImageClassification were not initialized from the model checkpoint at ./efficientnet_b0 and are newly initialized because the shapes did not match:\n",
      "- efficientnet.embeddings.convolution.weight: found shape torch.Size([32, 3, 3, 3]) in the checkpoint and torch.Size([32, 16, 3, 3]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 1280]) in the checkpoint and torch.Size([38, 1280]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([38]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1 - 訓練中:   2%|▏         | 26/1664 [00:15<16:34,  1.65it/s, loss=3.8703] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 180\u001b[39m\n\u001b[32m    177\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(config.output_dir / \u001b[33m'\u001b[39m\u001b[33mclassification_report.txt\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mw\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    178\u001b[39m         f.write(report)\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 144\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    142\u001b[39m best_val_acc = \u001b[32m0\u001b[39m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config.num_epochs):\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m     train_loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    145\u001b[39m     val_loss, val_acc = validate(model, val_loader, criterion, config.device, desc=\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - 驗證中\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    146\u001b[39m     scheduler.step()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 80\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, train_loader, criterion, optimizer, device, epoch)\u001b[39m\n\u001b[32m     78\u001b[39m images, labels = images.to(device), labels.to(device)\n\u001b[32m     79\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m loss = criterion(outputs, labels)\n\u001b[32m     82\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/shap/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/shap/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 87\u001b[39m, in \u001b[36mCategoricalCNN.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     85\u001b[39m     output = \u001b[38;5;28mself\u001b[39m.classifier(pooled_features)\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Hugging Face 模型\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedded_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m     output = outputs.logits\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/shap/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/shap/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/shap/lib/python3.12/site-packages/transformers/models/efficientnet/modeling_efficientnet.py:545\u001b[39m, in \u001b[36mEfficientNetForImageClassification.forward\u001b[39m\u001b[34m(self, pixel_values, labels, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    538\u001b[39m \u001b[33;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[32m    539\u001b[39m \u001b[33;03m    Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[32m    540\u001b[39m \u001b[33;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[32m    541\u001b[39m \u001b[33;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[32m    542\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    543\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m--> \u001b[39m\u001b[32m545\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mefficientnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    547\u001b[39m pooled_output = outputs.pooler_output \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;28;01melse\u001b[39;00m outputs[\u001b[32m1\u001b[39m]\n\u001b[32m    548\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.dropout(pooled_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/shap/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/shap/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/shap/lib/python3.12/site-packages/transformers/models/efficientnet/modeling_efficientnet.py:489\u001b[39m, in \u001b[36mEfficientNetModel.forward\u001b[39m\u001b[34m(self, pixel_values, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    485\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mYou have to specify pixel_values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    487\u001b[39m embedding_output = \u001b[38;5;28mself\u001b[39m.embeddings(pixel_values)\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[38;5;66;03m# Apply pooling\u001b[39;00m\n\u001b[32m    495\u001b[39m last_hidden_state = encoder_outputs[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/shap/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/shap/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/shap/lib/python3.12/site-packages/transformers/models/efficientnet/modeling_efficientnet.py:416\u001b[39m, in \u001b[36mEfficientNetEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    413\u001b[39m all_hidden_states = (hidden_states,) \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    415\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.blocks:\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m     hidden_states = \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    417\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[32m    418\u001b[39m         all_hidden_states += (hidden_states,)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/shap/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/shap/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/shap/lib/python3.12/site-packages/transformers/models/efficientnet/modeling_efficientnet.py:335\u001b[39m, in \u001b[36mEfficientNetBlock.forward\u001b[39m\u001b[34m(self, hidden_states)\u001b[39m\n\u001b[32m    333\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.expand_ratio != \u001b[32m1\u001b[39m:\n\u001b[32m    334\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.expansion(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdepthwise_conv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[38;5;66;03m# Squeeze and excite phase\u001b[39;00m\n\u001b[32m    338\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.squeeze_excite(hidden_states)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/shap/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/shap/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/shap/lib/python3.12/site-packages/transformers/models/efficientnet/modeling_efficientnet.py:185\u001b[39m, in \u001b[36mEfficientNetDepthwiseLayer.forward\u001b[39m\u001b[34m(self, hidden_states)\u001b[39m\n\u001b[32m    183\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.depthwise_conv(hidden_states)\n\u001b[32m    184\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.depthwise_norm(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdepthwise_act\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/shap/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/shap/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/shap/lib/python3.12/site-packages/torch/nn/modules/activation.py:432\u001b[39m, in \u001b[36mSiLU.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    431\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m432\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43msilu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/shap/lib/python3.12/site-packages/torch/nn/functional.py:2380\u001b[39m, in \u001b[36msilu\u001b[39m\u001b[34m(input, inplace)\u001b[39m\n\u001b[32m   2378\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[32m   2379\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch._C._nn.silu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2380\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43msilu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 將原始的 one-hot 編碼資料賦予新的變數名稱，以符合後續程式碼的命名\n",
    "train_Y_onehot = train_Y\n",
    "val_Y_onehot = val_Y\n",
    "test_Y_onehot = test_Y\n",
    "\n",
    "# 調整圖片大小以符合模型預期\n",
    "IMG_SIZE = 52\n",
    "\n",
    "def resize_ndarray(input_array, target_height, target_width):\n",
    "    # 使用 OpenCV 的 resize 函數進行調整\n",
    "    return cv2.resize(input_array, (target_width, target_height), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "print(f\"正在將晶圓圖大小從 (52, 52) 調整為 ({IMG_SIZE}, {IMG_SIZE})...\")\n",
    "resized_train_X = np.array([resize_ndarray(img, IMG_SIZE, IMG_SIZE) for img in tqdm(train_X, desc=\"調整訓練集大小\")])\n",
    "resized_val_X = np.array([resize_ndarray(img, IMG_SIZE, IMG_SIZE) for img in tqdm(val_X, desc=\"調整驗證集大小\")])\n",
    "resized_test_X = np.array([resize_ndarray(img, IMG_SIZE, IMG_SIZE) for img in tqdm(test_X, desc=\"調整測試集大小\")])\n",
    "print(\"大小調整完成。\")\n",
    "\n",
    "\n",
    "# --- 步驟 3: 更新 Dataset, Config 和訓練函式 ---\n",
    "\n",
    "# 更新後的 Dataset，確保返回正確的資料類型\n",
    "class WaferMapDataset(Dataset):\n",
    "    def __init__(self, mydata, mylabel):\n",
    "        # 將資料轉換為 LongTensor 以便嵌入層使用\n",
    "        self.data = torch.from_numpy(mydata).long()\n",
    "        # 將標籤轉換為 LongTensor 以便損失函式使用\n",
    "        self.labels = torch.from_numpy(mylabel).long()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 返回指定 index 的數據和標籤\n",
    "        return self.data[index], self.labels[index]\n",
    "\n",
    "# 更新後的 Config\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.seed = 42\n",
    "        self.image_size = IMG_SIZE\n",
    "        self.batch_size = 16  # 如果記憶體不足 (CUDA out of memory)，可以降低此值\n",
    "        self.num_workers = 0\n",
    "        self.num_epochs = 30\n",
    "        self.learning_rate = 5e-5\n",
    "        \n",
    "        # 本地模型路徑\n",
    "        self.model_path_local = './efficientnet_b0'\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device('cuda')\n",
    "        elif torch.backends.mps.is_available():\n",
    "            device = torch.device('mps')\n",
    "        else:\n",
    "            device = torch.device('cpu')\n",
    "        self.device = device\n",
    "        \n",
    "        self.output_dir = Path(\"./output\")\n",
    "        self.model_path = self.output_dir / \"Custom_EfficientNet_model.pth\"\n",
    "\n",
    "        # CategoricalCNN 所需的新參數\n",
    "        # 修正：晶圓圖資料中的獨特值數量 (0,1,2,3)\n",
    "        self.num_categories = 3\n",
    "        # 學習到的類別向量維度\n",
    "        self.embedding_dim = 16\n",
    "\n",
    "        # 類別和標籤\n",
    "        self.num_classes = 38\n",
    "        self.categories = [str(i) for i in range(self.num_classes)] # 用於分類報告\n",
    "\n",
    "\n",
    "# 您的 train_epoch 和 validate 函式無需修改，這裡為了完整性而包含進來\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    with tqdm(train_loader, desc=f'Epoch {epoch + 1} - 訓練中') as pbar:\n",
    "        for images, labels in pbar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, val_loader, criterion, device, desc='驗證中'):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with tqdm(val_loader, desc=desc) as pbar:\n",
    "        for images, labels in pbar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            accuracy = 100 * correct / total\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}', 'accuracy': f'{accuracy:.2f}%'})\n",
    "    return total_loss / len(val_loader), 100 * correct / total\n",
    "\n",
    "# --- 步驟 4: 更新主函式以使用新模型和資料 ---\n",
    "def main():\n",
    "    config = Config()\n",
    "    config.output_dir.mkdir(exist_ok=True)\n",
    "    print(f\"使用設備: {config.device}\")\n",
    "    print(f\"模型將儲存至: {config.model_path}\")\n",
    "    print(f\"使用本地 EfficientNet 模型: {config.model_path_local}\")\n",
    "\n",
    "    # 設定隨機種子以確保可重現性\n",
    "    torch.manual_seed(config.seed)\n",
    "    np.random.seed(config.seed)\n",
    "    random.seed(config.seed)\n",
    "\n",
    "    # 初始化 Dataset 和 DataLoader\n",
    "    train_dataset = WaferMapDataset(resized_train_X, train_Y)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=config.num_workers)\n",
    "\n",
    "    val_dataset = WaferMapDataset(resized_val_X, val_Y)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config.batch_size, num_workers=config.num_workers)\n",
    "\n",
    "    test_dataset = WaferMapDataset(resized_test_X, test_Y)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config.batch_size, num_workers=config.num_workers)\n",
    "\n",
    "    # --- 關鍵變更：實例化使用本地 EfficientNet 的 CategoricalCNN 模型 ---\n",
    "    model = CategoricalCNN(\n",
    "        num_classes=config.num_classes,\n",
    "        num_categories=config.num_categories,\n",
    "        embedding_dim=config.embedding_dim,\n",
    "        model_path=config.model_path_local\n",
    "    ).to(config.device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config.num_epochs)\n",
    "\n",
    "    best_val_acc = 0\n",
    "    for epoch in range(config.num_epochs):\n",
    "        train_loss = train_epoch(model, train_loader, criterion, optimizer, config.device, epoch)\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, config.device, desc=f'Epoch {epoch + 1} - 驗證中')\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f'\\nEpoch {epoch + 1}:')\n",
    "        print(f'訓練損失: {train_loss:.4f}')\n",
    "        print(f'驗證損失: {val_loss:.4f}, 驗證準確率: {val_acc:.2f}%')\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), config.model_path)\n",
    "            print(f'儲存最佳模型，驗證準確率: {val_acc:.2f}%')\n",
    "\n",
    "    print(\"\\n在測試集上評估最佳模型...\")\n",
    "    model.load_state_dict(torch.load(config.model_path))\n",
    "    test_loss, test_acc = validate(model, test_loader, criterion, config.device, desc='測試中')\n",
    "    print(f'\\n測試準確率: {test_acc:.2f}%')\n",
    "\n",
    "    print(\"\\n正在生成分類報告...\")\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader, desc='預測中'):\n",
    "            images = images.to(config.device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "\n",
    "    report = classification_report(all_labels, all_preds, target_names=config.categories, digits=4)\n",
    "    print('\\n分類報告:')\n",
    "    print(report)\n",
    "\n",
    "    with open(config.output_dir / 'classification_report.txt', 'w') as f:\n",
    "        f.write(report)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UBZp3QWn2Y5l"
   },
   "outputs": [],
   "source": [
    "# Check for shufflenet models\n",
    "shufflenet_models = timm.list_models('*shufflenet*')\n",
    "print(f\"Available ShuffleNet models: {shufflenet_models}\")\n",
    "\n",
    "# Check for mobilenet models (good alternatives)\n",
    "mobilenet_models = timm.list_models('*mobilenet*')\n",
    "print(f\"Available MobileNet models (first 5): {mobilenet_models[:5]}\")\n",
    "\n",
    "# Check for efficientnet models (also good alternatives)\n",
    "efficientnet_models = timm.list_models('*efficientnet*')\n",
    "print(f\"Available EfficientNet models (first 5): {efficientnet_models[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "354RQzIqODjk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HhsZf2HJMctV",
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 899128,
     "sourceId": 10404507,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30458,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "shap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
