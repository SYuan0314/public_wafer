{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyVK17WouLg8",
        "outputId": "947cfe76-540b-4627-b0c1-bfdf1711b950"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['label', 'image']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import random\n",
        "import matplotlib.image as img\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import timm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from tqdm import tqdm\n",
        "\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "random_seed = 42\n",
        "random.seed(random_seed)\n",
        "np.random.seed(random_seed)\n",
        "data = np.load(\"../Resize_Dataset.npz\")\n",
        "print(data.files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCnPeVo6uLg9",
        "outputId": "1257db04-1ce9-4440-9982-70f696010816"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "訓練集筆數: 79831\n",
            "驗證集筆數: 17106\n",
            "測試集筆數: 17108\n",
            "(79831, 65, 65)\n",
            "0  1  2  3  4  5  6  7\n",
            "1  0  1  0  0  0  1  0    4171\n",
            "0  1  1  0  0  0  1  0    2162\n",
            "1  0  0  0  1  0  0  0    2162\n",
            "0  1  1  0  1  0  1  0    2159\n",
            "1  0  0  0  0  0  0  0    2149\n",
            "0  1  0  0  1  0  1  0    2147\n",
            "            0  0  1  0    2146\n",
            "1  0  0  0  0  0  1  0    2136\n",
            "0  0  0  1  0  0  0  0    2118\n",
            "   1  1  0  0  0  0  0    2117\n",
            "1  0  1  0  0  0  0  0    2113\n",
            "0  0  1  0  0  0  1  0    2111\n",
            "   1  0  0  0  0  0  0    2108\n",
            "         1  0  0  1  0    2106\n",
            "         0  1  0  0  0    2104\n",
            "   0  0  1  1  0  0  0    2102\n",
            "            0  0  1  0    2101\n",
            "1  0  0  1  0  0  0  0    2099\n",
            "      1  0  1  0  0  0    2099\n",
            "      0  0  1  0  1  0    2099\n",
            "0  0  0  0  0  0  1  0    2098\n",
            "   1  0  1  1  0  0  0    2096\n",
            "      1  0  1  0  0  0    2093\n",
            "   0  0  0  1  0  0  0    2092\n",
            "      1  0  1  0  1  0    2091\n",
            "   1  0  1  1  0  1  0    2090\n",
            "   0  1  0  1  0  0  0    2088\n",
            "            0  0  0  0    2088\n",
            "1  0  0  1  1  0  0  0    2087\n",
            "                  1  0    2076\n",
            "0  0  0  0  0  0  0  0    2068\n",
            "   1  0  1  0  0  0  0    2062\n",
            "1  0  0  1  0  0  1  0    2051\n",
            "      1  0  1  0  1  0    2046\n",
            "0  0  0  0  1  0  1  0    2042\n",
            "         1  1  0  1  0    2040\n",
            "         0  0  0  0  1    1790\n",
            "               1  0  0     324\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "arrays = data['image']\n",
        "label = data['label']\n",
        "# print(arrays.shape)\n",
        "arrays = np.reshape(arrays,(-1,65*65))\n",
        "combine_array = np.concatenate([label,arrays],axis=1)\n",
        "# print(combine_array.shape)\n",
        "\n",
        "np.random.shuffle(combine_array)\n",
        "\n",
        "# 設定每個集合的大小\n",
        "total_rows = combine_array.shape[0]  # 總行數\n",
        "train_size = int(total_rows * 0.7)  # 訓練集大小 (70%)\n",
        "val_size = int(total_rows * 0.15)   # 驗證集大小 (15%)\n",
        "test_size = total_rows - train_size - val_size  # 測試集大小 (剩下的行數)\n",
        "\n",
        "# 分割數據集\n",
        "train_set = combine_array[:train_size]  # 前 70% 為訓練集\n",
        "val_set = combine_array[train_size:train_size + val_size]  # 接下來的 15% 為驗證集\n",
        "test_set = combine_array[train_size + val_size:]  # 剩下的 15% 為測試集\n",
        "\n",
        "print(\"訓練集筆數:\", train_set.shape[0])\n",
        "print(\"驗證集筆數:\", val_set.shape[0])\n",
        "print(\"測試集筆數:\", test_set.shape[0])\n",
        "\n",
        "train_X = train_set[:,8: ]\n",
        "train_Y = train_set[:, :8]\n",
        "val_X = val_set[:,8: ]\n",
        "val_Y = val_set[:, :8]\n",
        "test_X = test_set[:,8: ]\n",
        "test_Y = test_set[:, :8]\n",
        "\n",
        "train_X = np.reshape(train_X,(-1,65,65))\n",
        "val_X = np.reshape(val_X,(-1,65,65))\n",
        "test_X = np.reshape(test_X,(-1,65,65))\n",
        "\n",
        "print(train_X.shape)\n",
        "\n",
        "# 看各 label 個數\n",
        "print(pd.DataFrame(train_Y).value_counts())\n",
        "\n",
        "def decode_to_38bit(input_ndarray):\n",
        "    mapping = {\n",
        "        \"00000000\": 0,\n",
        "        \"10000000\": 1,\n",
        "        \"01000000\": 2,\n",
        "        \"00100000\": 3,\n",
        "        \"00010000\": 4,\n",
        "        \"00001000\": 5,\n",
        "        \"00000100\": 6,\n",
        "        \"00000010\": 7,\n",
        "        \"00000001\": 8,\n",
        "        \"10100000\": 9,\n",
        "        \"10010000\": 10,\n",
        "        \"10001000\": 11,\n",
        "        \"10000010\": 12,\n",
        "        \"01100000\": 13,\n",
        "        \"01010000\": 14,\n",
        "        \"01001000\": 15,\n",
        "        \"01000010\": 16,\n",
        "        \"00101000\": 17,\n",
        "        \"00100010\": 18,\n",
        "        \"00011000\": 19,\n",
        "        \"00010010\": 20,\n",
        "        \"00001010\": 21,\n",
        "        \"10101000\": 22,\n",
        "        \"10100010\": 23,\n",
        "        \"10011000\": 24,\n",
        "        \"10010010\": 25,\n",
        "        \"10001010\": 26,\n",
        "        \"01101000\": 27,\n",
        "        \"01100010\": 28,\n",
        "        \"01011000\": 29,\n",
        "        \"01010010\": 30,\n",
        "        \"01001010\": 31,\n",
        "        \"00101010\": 32,\n",
        "        \"00011010\": 33,\n",
        "        \"10101010\": 34,\n",
        "        \"10011010\": 35,\n",
        "        \"01101010\": 36,\n",
        "        \"01011010\": 37\n",
        "    }\n",
        "    n = input_ndarray.shape[0]  # 行數\n",
        "    decoded_ndarray = np.zeros(n, dtype=int)\n",
        "    # 逐行解碼\n",
        "    for i, row in enumerate(input_ndarray):\n",
        "        # 將每一行轉換為字典的鍵，並獲得對應的索引\n",
        "        input_8bit = ''.join(row.astype(str))  # 把整數轉為字串並拼接\n",
        "        number = mapping[input_8bit]  # 獲取對應的數字索引\n",
        "        decoded_ndarray[i] = number\n",
        "    return decoded_ndarray\n",
        "\n",
        "# label 轉成 one hot encoding\n",
        "train_Y = decode_to_38bit(train_Y)\n",
        "val_Y = decode_to_38bit(val_Y)\n",
        "test_Y = decode_to_38bit(test_Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJc3fptuuLhG"
      },
      "outputs": [],
      "source": [
        "# 自定義一個簡單的 Dataset 類\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self,mydata,mylabel):\n",
        "        # 定義數據\n",
        "        self.data = mydata\n",
        "        self.labels = mylabel\n",
        "\n",
        "    def __len__(self):\n",
        "        # 返回數據集的大小\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # 返回指定 index 的數據和標籤\n",
        "        return self.data[index], self.labels[index]\n",
        "\n",
        "# --- 步驟 1: 定義一個更穩健的 CategoricalCNN 模型 ---\n",
        "# 這個版本能更可靠地自動獲取骨幹網路的輸出特徵數量\n",
        "class CategoricalCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    專為二維類別資料（如晶圓圖）設計的 CNN。\n",
        "    它使用嵌入層從類別創建特徵向量，然後將其饋送到標準的 CNN 骨幹網路。\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes, num_categories=3, embedding_dim=16, backbone_name='efficientnet_b0'):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.num_categories = num_categories\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "        # 1. 嵌入層，用於學習晶圓圖中各個類別的向量表示\n",
        "        # 輸入是類別索引 (0, 1, 2)，輸出是密集向量\n",
        "        self.embedding = nn.Embedding(num_embeddings=num_categories, embedding_dim=embedding_dim)\n",
        "\n",
        "        # 2. 從 timm 中選擇一個現代化的 CNN 骨幹網路\n",
        "        self.backbone = timm.create_model(\n",
        "            backbone_name,\n",
        "            pretrained=False,        # 盡可能使用預訓練權重\n",
        "            features_only=True,     # 我們只需要特徵提取部分\n",
        "            in_chans=embedding_dim  # 關鍵：將骨幹網路的輸入通道數修改為嵌入維度\n",
        "        )\n",
        "\n",
        "        # 3. 使用 timm 的 feature_info 來安全地獲取骨幹網路的輸出特徵維度\n",
        "        num_features = self.backbone.feature_info.channels(-1)\n",
        "\n",
        "        # 4. 分類頭\n",
        "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.classifier = nn.Linear(num_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x 的輸入形狀: (batch_size, height, width)，其值為整數類別\n",
        "        # `embedding` 層需要 LongTensor\n",
        "        x = x.long()\n",
        "\n",
        "        # 通過嵌入層處理\n",
        "        # 輸出形狀: (batch_size, height, width, embedding_dim)\n",
        "        embedded_x = self.embedding(x)\n",
        "\n",
        "        # 調整維度以匹配 CNN 的輸入格式 (N, C, H, W)\n",
        "        # 輸出形狀: (batch_size, embedding_dim, height, width)\n",
        "        embedded_x = embedded_x.permute(0, 3, 1, 2)\n",
        "\n",
        "        # 從骨幹網路提取特徵\n",
        "        # features_only=True 的輸出是一個包含不同階段特徵圖的列表\n",
        "        features = self.backbone(embedded_x)\n",
        "\n",
        "        # 我們使用最後一個、包含最豐富資訊的特徵圖進行分類\n",
        "        last_feature_map = features[-1]\n",
        "\n",
        "        # 應用全域池化和最終的分類層\n",
        "        pooled_features = self.global_pool(last_feature_map).flatten(1)\n",
        "        output = self.classifier(pooled_features)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OP6pwHxJ84Po",
        "outputId": "17b2a464-32de-4902-f42a-0be65ff800b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在將晶圓圖大小從 (52, 52) 調整為 (65, 65)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "調整訓練集大小: 100%|██████████| 79831/79831 [00:01<00:00, 65373.30it/s]\n",
            "調整驗證集大小: 100%|██████████| 17106/17106 [00:00<00:00, 138640.61it/s]\n",
            "調整測試集大小: 100%|██████████| 17108/17108 [00:00<00:00, 129354.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "大小調整完成。\n",
            "使用設備: cuda\n",
            "模型將儲存至: /content/drive/MyDrive/Colab Notebooks/save/mobilenet_edgetpu_v2_l.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1 - 訓練中: 100%|██████████| 4990/4990 [05:20<00:00, 15.57it/s, loss=1.3257]\n",
            "Epoch 1 - 驗證中: 100%|██████████| 1070/1070 [00:24<00:00, 43.37it/s, loss=1.0512, accuracy=76.27%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1:\n",
            "訓練損失: 2.0250\n",
            "驗證損失: 0.7346, 驗證準確率: 76.27%\n",
            "儲存最佳模型，驗證準確率: 76.27%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2 - 訓練中: 100%|██████████| 4990/4990 [04:45<00:00, 17.46it/s, loss=0.5642]\n",
            "Epoch 2 - 驗證中: 100%|██████████| 1070/1070 [00:23<00:00, 45.90it/s, loss=2.5086, accuracy=85.58%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 2:\n",
            "訓練損失: 0.7036\n",
            "驗證損失: 0.4475, 驗證準確率: 85.58%\n",
            "儲存最佳模型，驗證準確率: 85.58%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3 - 訓練中: 100%|██████████| 4990/4990 [04:45<00:00, 17.46it/s, loss=0.6108]\n",
            "Epoch 3 - 驗證中: 100%|██████████| 1070/1070 [00:22<00:00, 47.21it/s, loss=1.0049, accuracy=88.05%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 3:\n",
            "訓練損失: 0.4497\n",
            "驗證損失: 0.3660, 驗證準確率: 88.05%\n",
            "儲存最佳模型，驗證準確率: 88.05%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4 - 訓練中: 100%|██████████| 4990/4990 [04:48<00:00, 17.32it/s, loss=0.6556]\n",
            "Epoch 4 - 驗證中: 100%|██████████| 1070/1070 [00:23<00:00, 45.49it/s, loss=1.4769, accuracy=90.30%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 4:\n",
            "訓練損失: 0.3379\n",
            "驗證損失: 0.2955, 驗證準確率: 90.30%\n",
            "儲存最佳模型，驗證準確率: 90.30%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5 - 訓練中: 100%|██████████| 4990/4990 [04:43<00:00, 17.59it/s, loss=1.3557]\n",
            "Epoch 5 - 驗證中: 100%|██████████| 1070/1070 [00:23<00:00, 45.61it/s, loss=3.7394, accuracy=91.75%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 5:\n",
            "訓練損失: 0.2676\n",
            "驗證損失: 0.2721, 驗證準確率: 91.75%\n",
            "儲存最佳模型，驗證準確率: 91.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6 - 訓練中: 100%|██████████| 4990/4990 [04:38<00:00, 17.90it/s, loss=0.3430]\n",
            "Epoch 6 - 驗證中: 100%|██████████| 1070/1070 [00:23<00:00, 46.49it/s, loss=3.4074, accuracy=91.65%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 6:\n",
            "訓練損失: 0.2144\n",
            "驗證損失: 0.2621, 驗證準確率: 91.65%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7 - 訓練中: 100%|██████████| 4990/4990 [04:41<00:00, 17.74it/s, loss=0.0414]\n",
            "Epoch 7 - 驗證中: 100%|██████████| 1070/1070 [00:24<00:00, 44.27it/s, loss=0.2852, accuracy=92.78%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 7:\n",
            "訓練損失: 0.1775\n",
            "驗證損失: 0.2320, 驗證準確率: 92.78%\n",
            "儲存最佳模型，驗證準確率: 92.78%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8 - 訓練中: 100%|██████████| 4990/4990 [04:49<00:00, 17.24it/s, loss=0.1864]\n",
            "Epoch 8 - 驗證中: 100%|██████████| 1070/1070 [00:24<00:00, 43.73it/s, loss=1.2007, accuracy=92.12%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 8:\n",
            "訓練損失: 0.1458\n",
            "驗證損失: 0.2581, 驗證準確率: 92.12%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9 - 訓練中: 100%|██████████| 4990/4990 [04:53<00:00, 17.01it/s, loss=0.0076]\n",
            "Epoch 9 - 驗證中: 100%|██████████| 1070/1070 [00:24<00:00, 44.21it/s, loss=1.1860, accuracy=91.08%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 9:\n",
            "訓練損失: 0.1176\n",
            "驗證損失: 0.3242, 驗證準確率: 91.08%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10 - 訓練中:  34%|███▍      | 1711/4990 [01:41<02:53, 18.95it/s, loss=0.0090]"
          ]
        }
      ],
      "source": [
        "# 將原始的 one-hot 編碼資料賦予新的變數名稱，以符合後續程式碼的命名\n",
        "train_Y_onehot = train_Y\n",
        "val_Y_onehot = val_Y\n",
        "test_Y_onehot = test_Y\n",
        "\n",
        "# 使用 OpenCV 的 resize 函數進行調整\n",
        "def resize_ndarray(input_array, target_height, target_width):\n",
        "    return cv2.resize(input_array, (target_width, target_height), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "IMG_SIZE = 65\n",
        "print(f\"正在將晶圓圖大小從 (52, 52) 調整為 ({IMG_SIZE}, {IMG_SIZE})...\")\n",
        "resized_train_X = np.array([resize_ndarray(img, IMG_SIZE, IMG_SIZE) for img in tqdm(train_X, desc=\"調整訓練集大小\")])\n",
        "resized_val_X = np.array([resize_ndarray(img, IMG_SIZE, IMG_SIZE) for img in tqdm(val_X, desc=\"調整驗證集大小\")])\n",
        "resized_test_X = np.array([resize_ndarray(img, IMG_SIZE, IMG_SIZE) for img in tqdm(test_X, desc=\"調整測試集大小\")])\n",
        "print(\"大小調整完成。\")\n",
        "\n",
        "# --- 步驟 3: 更新 Dataset, Config 和訓練函式 ---\n",
        "# 更新後的 Dataset，確保返回正確的資料類型\n",
        "class WaferMapDataset(Dataset):\n",
        "    def __init__(self, mydata, mylabel):\n",
        "        # 將資料轉換為 LongTensor 以便嵌入層使用\n",
        "        self.data = torch.from_numpy(mydata).long()\n",
        "        # 將標籤轉換為 LongTensor 以便損失函式使用\n",
        "        self.labels = torch.from_numpy(mylabel).long()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # 返回指定 index 的數據和標籤\n",
        "        return self.data[index], self.labels[index]\n",
        "\n",
        "# 更新後的 Config\n",
        "class Config:\n",
        "    def __init__(self):\n",
        "        self.seed = 42\n",
        "        self.image_size = 52\n",
        "        self.batch_size = 16  # 如果記憶體不足 (CUDA out of memory)，可以降低此值\n",
        "        self.num_workers = 2\n",
        "        self.num_epochs = 30\n",
        "        self.learning_rate = 5e-5\n",
        "        # 建議從一個較小的模型開始，以加快實驗速度\n",
        "        self.model_name = 'mobilenet_edgetpu_v2_l'\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.output_dir = Path(\"./save\")\n",
        "        self.model_path = self.output_dir / f\"{self.model_name}.pth\"\n",
        "\n",
        "        # CategoricalCNN 所需的新參數\n",
        "        # 您的晶圓圖資料中的獨特值數量 (例如 0, 1, 2)\n",
        "        # 如果不確定，可以執行 np.unique(train_X) 來檢查\n",
        "        self.num_categories = 38\n",
        "        # 學習到的類別向量維度，16 是一個不錯的起點\n",
        "        self.embedding_dim = 16\n",
        "\n",
        "        # 類別和標籤\n",
        "        self.num_classes = 38\n",
        "        self.categories = [str(i) for i in range(self.num_classes)] # 用於分類報告\n",
        "\n",
        "# 您的 train_epoch 和 validate 函式無需修改，這裡為了完整性而包含進來\n",
        "def train_epoch(model, train_loader, criterion, optimizer, device, epoch):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    with tqdm(train_loader, desc=f'Epoch {epoch + 1} - 訓練中') as pbar:\n",
        "        for images, labels in pbar:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate(model, val_loader, criterion, device, desc='驗證中'):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with tqdm(val_loader, desc=desc) as pbar:\n",
        "        for images, labels in pbar:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            accuracy = 100 * correct / total\n",
        "            pbar.set_postfix({'loss': f'{loss.item():.4f}', 'accuracy': f'{accuracy:.2f}%'})\n",
        "    return total_loss / len(val_loader), 100 * correct / total\n",
        "\n",
        "# --- 步驟 4: 更新主函式以使用新模型和資料 ---\n",
        "def main():\n",
        "    config = Config()\n",
        "    config.output_dir.mkdir(exist_ok=True)\n",
        "    print(f\"使用設備: {config.device}\")\n",
        "    print(f\"模型將儲存至: {config.model_path}\")\n",
        "\n",
        "    # 設定隨機種子以確保可重現性\n",
        "    torch.manual_seed(config.seed)\n",
        "    np.random.seed(config.seed)\n",
        "    random.seed(config.seed)\n",
        "\n",
        "    # 初始化 Dataset 和 DataLoader\n",
        "    train_dataset = WaferMapDataset(resized_train_X, train_Y)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=config.num_workers)\n",
        "\n",
        "    val_dataset = WaferMapDataset(resized_val_X, val_Y)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=config.batch_size, num_workers=config.num_workers)\n",
        "\n",
        "    test_dataset = WaferMapDataset(resized_test_X, test_Y)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=config.batch_size, num_workers=config.num_workers)\n",
        "\n",
        "    # --- 關鍵變更：實例化新的 CategoricalCNN 模型 ---\n",
        "    model = CategoricalCNN(\n",
        "        num_classes=config.num_classes,\n",
        "        num_categories=config.num_categories,\n",
        "        embedding_dim=config.embedding_dim,\n",
        "        backbone_name=config.model_name\n",
        "    ).to(config.device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config.num_epochs)\n",
        "\n",
        "    best_val_acc = 0\n",
        "    for epoch in range(config.num_epochs):\n",
        "        train_loss = train_epoch(model, train_loader, criterion, optimizer, config.device, epoch)\n",
        "        val_loss, val_acc = validate(model, val_loader, criterion, config.device, desc=f'Epoch {epoch + 1} - 驗證中')\n",
        "        scheduler.step()\n",
        "\n",
        "        print(f'\\nEpoch {epoch + 1}:')\n",
        "        print(f'訓練損失: {train_loss:.4f}')\n",
        "        print(f'驗證損失: {val_loss:.4f}, 驗證準確率: {val_acc:.2f}%')\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), config.model_path)\n",
        "            print(f'儲存最佳模型，驗證準確率: {val_acc:.2f}%')\n",
        "\n",
        "    print(\"\\n在測試集上評估最佳模型...\")\n",
        "    model.load_state_dict(torch.load(config.model_path))\n",
        "    test_loss, test_acc = validate(model, test_loader, criterion, config.device, desc='測試中')\n",
        "    print(f'\\n測試準確率: {test_acc:.2f}%')\n",
        "\n",
        "    print(\"\\n正在生成分類報告...\")\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(test_loader, desc='預測中'):\n",
        "            images = images.to(config.device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.numpy())\n",
        "\n",
        "    report = classification_report(all_labels, all_preds, target_names=config.categories, digits=4)\n",
        "    print('\\n分類報告:')\n",
        "    print(report)\n",
        "\n",
        "    with open(config.output_dir / 'classification_report.txt', 'w') as f:\n",
        "        f.write(report)\n",
        "\n",
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZSxxcAkbT16"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 899128,
          "sourceId": 10404507,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30458,
      "isGpuEnabled": true,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
